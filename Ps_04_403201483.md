```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy.stats import ttest_ind
from sklearn.preprocessing import LabelEncoder
from numpy.linalg import inv
from sklearn.preprocessing import StandardScaler
from itertools import combinations
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
```

# Load Data


```python
data = pd.read_csv("Employee_Performance.csv")
print("All Columns:", data.columns.tolist())

```

    All Columns: ['employee_id', 'department', 'gender', 'age', 'job_title', 'hire_date', 'years_at_company', 'education_level', 'work_hours_per_week', 'projects_handled', 'overtime_hours', 'sick_days', 'remote_work_frequency', 'team_size', 'training_hours', 'promotions', 'edu_level', 'monthly_salary', 'performance_score', 'employee_satisfaction_score', 'resigned', 'treatment', 'team_id', 'base_team_rate', 'team_training_rate', 'team_treatment', 'team_avg', 'median_team_rate']
    

# Q_2_part_a




```python

treatment_dist = data['treatment'].value_counts(normalize=True) * 100
print("درصد کارمندانی که آموزش دیدن (treatment=1): {:.2f}%".format(treatment_dist[1]))
print("درصد کارمندانی که آموزش ندیدن (treatment=0): {:.2f}%".format(treatment_dist[0]))

covariates = ['age', 'years_at_company', 'projects_handled', 'remote_work_frequency']
X = data[covariates]
y = data['treatment']

model = LogisticRegression(max_iter=1000)
model.fit(X, y)
data['propensity_score'] = model.predict_proba(X)[:, 1] 

plt.figure(figsize=(10, 6))
sns.histplot(data=data[data['treatment'] == 1], x='propensity_score', color='blue', label='Treated', alpha=0.5)
sns.histplot(data=data[data['treatment'] == 0], x='propensity_score', color='red', label='Control', alpha=0.5)
plt.title('Distribution of Propensity Scores by Treatment Group')
plt.xlabel('Propensity Score')
plt.ylabel('Count')
plt.legend()
plt.show()

print("\n Summaryy stat:")
print(data.groupby('treatment')['propensity_score'].describe())
```

    درصد کارمندانی که آموزش دیدن (treatment=1): 49.64%
    درصد کارمندانی که آموزش ندیدن (treatment=0): 50.36%
    


    
![png](output_4_1.png)
    


    
     Summaryy stat:
                 count      mean       std       min       25%       50%  \
    treatment                                                              
    0          50363.0  0.485530  0.073044  0.305107  0.428692  0.482300   
    1          49637.0  0.507369  0.073132  0.313515  0.451073  0.510298   
    
                    75%       max  
    treatment                      
    0          0.541691  0.689337  
    1          0.564790  0.691340  
    

# Q_2 Part_b


```python

covariates = ['age', 'years_at_company', 'projects_handled', 'remote_work_frequency']

treated = data[data['treatment'] == 1]
control = data[data['treatment'] == 0]

nn = NearestNeighbors(n_neighbors=1, radius=0.2)
nn.fit(control[['propensity_score']])
distances, indices = nn.kneighbors(treated[['propensity_score']])

matches = pd.DataFrame({'treated_idx': treated.index, 'control_idx': control.index[indices.flatten()], 'distance': distances.flatten()})
matches = matches[matches['distance'] <= 0.2]
matched_data = pd.concat([data.loc[matches['treated_idx']], data.loc[matches['control_idx']]])

def calculate_smd(data, treated_col, covariates):
    smd = {}
    for cov in covariates:
        treated_mean = data[data[treated_col] == 1][cov].mean()
        control_mean = data[data[treated_col] == 0][cov].mean()
        pooled_std = np.sqrt((data[data[treated_col] == 1][cov].var() + data[data[treated_col] == 0][cov].var()) / 2)
        smd[cov] = (treated_mean - control_mean) / pooled_std
    return smd

smd_before = calculate_smd(data, 'treatment', covariates)
smd_after = calculate_smd(matched_data, 'treatment', covariates)

print("\nSMD قبل از تطبیق:", smd_before)
print("SMD بعد از تطبیق:", smd_after)


plt.figure(figsize=(10, 6))
sns.histplot(data=matched_data[matched_data['treatment'] == 1], x='propensity_score', color='blue', label='Treated', alpha=0.5)
sns.histplot(data=matched_data[matched_data['treatment'] == 0], x='propensity_score', color='red', label='Control', alpha=0.5)
plt.title('Propensity Score Distribution After Matching')
plt.xlabel('Propensity Score')
plt.ylabel('Count')
plt.legend()
plt.show()

smd_df = pd.DataFrame({
    'Covariate': covariates,
    'SMD Before': [smd_before[cov] for cov in covariates],
    'SMD After': [smd_after[cov] for cov in covariates]
})

plt.figure(figsize=(10, 6))
plt.plot(smd_df['SMD Before'], smd_df['Covariate'], 'o', label='Before Matching', color='red')
plt.plot(smd_df['SMD After'], smd_df['Covariate'], 'o', label='After Matching', color='blue')
plt.axvline(x=0.1, color='gray', linestyle='--', label='SMD Threshold (0.1)')
plt.axvline(x=-0.1, color='gray', linestyle='--')
plt.title('Standardized Mean Differences Before and After Matching')
plt.xlabel('SMD')
plt.ylabel('Covariate')
plt.legend()
plt.show()
```

    
    SMD قبل از تطبیق: {'age': 0.10306627028167818, 'years_at_company': 0.08392089078883404, 'projects_handled': 0.2631476840429122, 'remote_work_frequency': -0.04206628339798315}
    SMD بعد از تطبیق: {'age': 0.00023409380024455825, 'years_at_company': 0.000980984662231257, 'projects_handled': 9.382634697015159e-05, 'remote_work_frequency': 0.00314485896936024}
    


    
![png](output_6_1.png)
    



    
![png](output_6_2.png)
    


# Q_2 part c


```python

covariates = ['age', 'years_at_company', 'projects_handled', 'remote_work_frequency']

treated_salary = matched_data[matched_data['treatment'] == 1]['monthly_salary'].mean()
control_salary = matched_data[matched_data['treatment'] == 0]['monthly_salary'].mean()
att_diff = treated_salary - control_salary
print("\nATT from Mean Difference: {:.2f}".format(att_diff))

X_matched = matched_data[covariates]
X_matched = sm.add_constant(X_matched)
y_matched = matched_data['monthly_salary']
ols_model = sm.OLS(y_matched, X_matched).fit()
print(ols_model.summary())

print("\nComparison and Interpretation:")
print(f"ATT from Mean Difference: {att_diff:.2f}")
print(f"R-squared of the Regression: {ols_model.rsquared:.3f}")

```

    
    ATT from Mean Difference: 16.47
                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:         monthly_salary   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                 -0.000
    Method:                 Least Squares   F-statistic:                    0.6977
    Date:                Thu, 03 Apr 2025   Prob (F-statistic):              0.593
    Time:                        09:51:22   Log-Likelihood:            -8.5881e+05
    No. Observations:               99274   AIC:                         1.718e+06
    Df Residuals:                   99269   BIC:                         1.718e+06
    Df Model:                           4                                         
    Covariance Type:            nonrobust                                         
    =========================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
    -----------------------------------------------------------------------------------------
    const                  6537.8448     20.953    312.025      0.000    6496.777    6578.912
    age                       0.4789      0.392      1.220      0.222      -0.290       1.248
    years_at_company         -1.1294      1.538     -0.734      0.463      -4.144       1.886
    projects_handled          0.0340      0.305      0.111      0.911      -0.564       0.632
    remote_work_frequency     0.1098      0.125      0.877      0.381      -0.136       0.355
    ==============================================================================
    Omnibus:                    11431.757   Durbin-Watson:                   2.002
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3199.593
    Skew:                           0.090   Prob(JB):                         0.00
    Kurtosis:                       2.139   Cond. No.                         358.
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Comparison and Interpretation:
    ATT from Mean Difference: 16.47
    R-squared of the Regression: 0.000
    


```python
treated_salary = matched_data[matched_data['treatment'] == 1]['monthly_salary']
control_salary = matched_data[matched_data['treatment'] == 0]['monthly_salary']
t_stat, p_val = ttest_ind(treated_salary, control_salary)
print(f"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}")
```

    t-statistic: 1.876, p-value: 0.061
    

# Q_2 Part d


```python
X_full = data[covariates + ['treatment']]
X_full = sm.add_constant(X_full)
y_full = data['monthly_salary']
full_model = sm.OLS(y_full, X_full).fit()
print(full_model.summary())

print("\nComparison with ATT from Matching:")
print(f"Coefficient of treatment in the full sample: {full_model.params['treatment']:.2f}")
print(f"ATT from matching: {att_diff:.2f}")
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:         monthly_salary   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                  0.000
    Method:                 Least Squares   F-statistic:                     2.731
    Date:                Thu, 03 Apr 2025   Prob (F-statistic):             0.0180
    Time:                        09:51:22   Log-Likelihood:            -8.6477e+05
    No. Observations:              100000   AIC:                         1.730e+06
    Df Residuals:                   99994   BIC:                         1.730e+06
    Df Model:                           5                                         
    Covariance Type:            nonrobust                                         
    =========================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
    -----------------------------------------------------------------------------------------
    const                  6535.3445     20.437    319.778      0.000    6495.288    6575.401
    age                       0.2707      0.388      0.697      0.486      -0.490       1.032
    years_at_company         -0.4089      1.521     -0.269      0.788      -3.389       2.572
    projects_handled         -0.3222      0.304     -1.060      0.289      -0.918       0.273
    remote_work_frequency    -0.0055      0.123     -0.044      0.965      -0.247       0.236
    treatment                31.1456      8.816      3.533      0.000      13.866      48.425
    ==============================================================================
    Omnibus:                    11459.587   Durbin-Watson:                   1.996
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3241.091
    Skew:                           0.099   Prob(JB):                         0.00
    Kurtosis:                       2.141   Cond. No.                         351.
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Comparison with ATT from Matching:
    Coefficient of treatment in the full sample: 31.15
    ATT from matching: 16.47
    

# Q_2 Part e


```python
X_hours = data[['training_hours'] + covariates]
X_hours = sm.add_constant(X_hours)
y_hours = data['monthly_salary']
hours_model = sm.OLS(y_hours, X_hours).fit()
print(hours_model.summary())

data['training_hours_sq'] = data['training_hours'] ** 2
X_hours_sq = data[['training_hours', 'training_hours_sq'] + covariates]
X_hours_sq = sm.add_constant(X_hours_sq)
hours_sq_model = sm.OLS(y_hours, X_hours_sq).fit()
print("\nRegression with training_hours and its square:")
print(hours_sq_model.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:         monthly_salary   R-squared:                       0.002
    Model:                            OLS   Adj. R-squared:                  0.002
    Method:                 Least Squares   F-statistic:                     34.66
    Date:                Thu, 03 Apr 2025   Prob (F-statistic):           1.53e-35
    Time:                        09:51:23   Log-Likelihood:            -8.6469e+05
    No. Observations:              100000   AIC:                         1.729e+06
    Df Residuals:                   99994   BIC:                         1.729e+06
    Df Model:                           5                                         
    Covariance Type:            nonrobust                                         
    =========================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
    -----------------------------------------------------------------------------------------
    const                  6447.2667     21.573    298.856      0.000    6404.984    6489.550
    training_hours            1.9783      0.151     13.120      0.000       1.683       2.274
    age                       0.3316      0.387      0.856      0.392      -0.428       1.091
    years_at_company         -0.2376      1.518     -0.157      0.876      -3.213       2.738
    projects_handled         -0.1905      0.301     -0.633      0.527      -0.781       0.400
    remote_work_frequency    -0.0231      0.123     -0.187      0.851      -0.265       0.218
    ==============================================================================
    Omnibus:                    11635.179   Durbin-Watson:                   1.995
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3264.073
    Skew:                           0.099   Prob(JB):                         0.00
    Kurtosis:                       2.137   Cond. No.                         445.
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Regression with training_hours and its square:
                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:         monthly_salary   R-squared:                       0.002
    Model:                            OLS   Adj. R-squared:                  0.002
    Method:                 Least Squares   F-statistic:                     32.83
    Date:                Thu, 03 Apr 2025   Prob (F-statistic):           9.25e-40
    Time:                        09:51:23   Log-Likelihood:            -8.6468e+05
    No. Observations:              100000   AIC:                         1.729e+06
    Df Residuals:                   99993   BIC:                         1.729e+06
    Df Model:                           6                                         
    Covariance Type:            nonrobust                                         
    =========================================================================================
                                coef    std err          t      P>|t|      [0.025      0.975]
    -----------------------------------------------------------------------------------------
    const                  6401.3543     23.551    271.813      0.000    6355.195    6447.513
    training_hours            4.7832      0.597      8.015      0.000       3.614       5.953
    training_hours_sq        -0.0283      0.006     -4.858      0.000      -0.040      -0.017
    age                       0.3378      0.387      0.872      0.383      -0.421       1.097
    years_at_company         -0.1975      1.518     -0.130      0.896      -3.173       2.778
    projects_handled         -0.2016      0.301     -0.670      0.503      -0.792       0.388
    remote_work_frequency    -0.0235      0.123     -0.190      0.849      -0.265       0.218
    ==============================================================================
    Omnibus:                    11680.121   Durbin-Watson:                   1.995
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3270.437
    Skew:                           0.099   Prob(JB):                         0.00
    Kurtosis:                       2.137   Cond. No.                     2.39e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.39e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

# Question 3

# Part a



```python
print(data['education_level'].unique())
```

    ['High School' 'Bachelor' 'Master' 'PhD']
    


```python
q25 = data['remote_work_frequency'].quantile(0.25)
q75 = data['remote_work_frequency'].quantile(0.75)

data['treatment'] = np.where(data['remote_work_frequency'] >= q75, 1, 
                             np.where(data['remote_work_frequency'] <= q25, 0, np.nan))
data = data.dropna(subset=['treatment'])   

treatment_dist = data['treatment'].value_counts(normalize=True) * 100
print("  High Freq     (treatment=1): {:.2f}%".format(treatment_dist[1]))
print("     Low Freq  (treatment=0): {:.2f}%".format(treatment_dist[0]))

label_encoder = LabelEncoder()
data['education_level_encoded'] = label_encoder.fit_transform(data['education_level'])

print("  education_level:")
print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

covariates = ['age', 'education_level_encoded', 'years_at_company', 'overtime_hours']
X = data[covariates]
y = data['treatment']
model = LogisticRegression(max_iter=1000)
model.fit(X, y)
data['propensity_score'] = model.predict_proba(X)[:, 1]

summary_stats = data.groupby('treatment')['propensity_score'].describe()
print("\n Summart Stat:")
display(summary_stats)


plt.figure(figsize=(10, 6))
sns.histplot(data=data[data['treatment'] == 1], x='propensity_score', color='blue', label='Treated', alpha=0.5)
sns.histplot(data=data[data['treatment'] == 0], x='propensity_score', color='red', label='Control', alpha=0.5)
plt.title('Distribution of Propensity Scores by Treatment Group')
plt.xlabel('Propensity Score')
plt.ylabel('Count')
plt.legend()
plt.savefig('propensity_score_distribution_q3a.png', dpi=300, bbox_inches='tight')
plt.show()
```

      High Freq     (treatment=1): 50.09%
         Low Freq  (treatment=0): 49.91%
      education_level:
    {'Bachelor': 0, 'High School': 1, 'Master': 2, 'PhD': 3}
    
     Summart Stat:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>treatment</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.0</th>
      <td>40072.0</td>
      <td>0.500901</td>
      <td>0.004033</td>
      <td>0.490038</td>
      <td>0.497927</td>
      <td>0.500896</td>
      <td>0.503892</td>
      <td>0.511849</td>
    </tr>
    <tr>
      <th>1.0</th>
      <td>40222.0</td>
      <td>0.500967</td>
      <td>0.004057</td>
      <td>0.490038</td>
      <td>0.497924</td>
      <td>0.500994</td>
      <td>0.504006</td>
      <td>0.511878</td>
    </tr>
  </tbody>
</table>
</div>



    
![png](output_17_2.png)
    


# Q_3 Part b


```python
def nearest_neighbor_matching(data, caliper=0.2):
    treated = data[data['treatment'] == 1].copy()
    control = data[data['treatment'] == 0].copy()
    
    matched_pairs = []
    used_control_indices = set()
    
    for treated_idx, treated_row in treated.iterrows():
        control['ps_diff'] = abs(control['propensity_score'] - treated_row['propensity_score'])
        control_within_caliper = control[control['ps_diff'] <= caliper]
        control_within_caliper = control_within_caliper[~control_within_caliper.index.isin(used_control_indices)]
        
        if not control_within_caliper.empty:
            closest_control = control_within_caliper.loc[control_within_caliper['ps_diff'].idxmin()]
            matched_pairs.append((treated_idx, closest_control.name))
            used_control_indices.add(closest_control.name)
    
    treated_indices = [pair[0] for pair in matched_pairs]
    control_indices = [pair[1] for pair in matched_pairs]
    matched_data = pd.concat([data.loc[treated_indices], data.loc[control_indices]])
    return matched_data

matched_data = nearest_neighbor_matching(data, caliper=0.2)
print(" Number observation before matching:", len(matched_data))

def calculate_smd(data, matched_data, covariates):
    smd_before = {}
    smd_after = {}
    
    for cov in covariates:
        treated_mean = data[data['treatment'] == 1][cov].mean()
        control_mean = data[data['treatment'] == 0][cov].mean()
        treated_var = data[data['treatment'] == 1][cov].var()
        control_var = data[data['treatment'] == 0][cov].var()
        smd_before[cov] = (treated_mean - control_mean) / np.sqrt((treated_var + control_var) / 2)
        
        treated_mean_matched = matched_data[matched_data['treatment'] == 1][cov].mean()
        control_mean_matched = matched_data[matched_data['treatment'] == 0][cov].mean()
        treated_var_matched = matched_data[matched_data['treatment'] == 1][cov].var()
        control_var_matched = matched_data[matched_data['treatment'] == 0][cov].var()
        smd_after[cov] = (treated_mean_matched - control_mean_matched) / np.sqrt((treated_var_matched + control_var_matched) / 2)
    
    return smd_before, smd_after

smd_before, smd_after = calculate_smd(data, matched_data, covariates)
print("\nSMD BEfore matching:", smd_before)
print("SMD After Matching:", smd_after)

def p_test(data, matched_data, covariates):
    p_before = {}
    p_after = {}
    
    for cov in covariates:
        treated = data[data['treatment'] == 1][cov]
        control = data[data['treatment'] == 0][cov]
        t_stat, p_val = ttest_ind(treated, control, equal_var=False)
        p_before[cov] = p_val
        
        treated_matched = matched_data[matched_data['treatment'] == 1][cov]
        control_matched = matched_data[matched_data['treatment'] == 0][cov]
        t_stat, p_val = ttest_ind(treated_matched, control_matched, equal_var=False)
        p_after[cov] = p_val
    
    return p_before, p_after

p_before, p_after = p_test(data, matched_data, covariates)
print("\nP-values Before Matching:", p_before)
print("P-values  After matching :", p_after)

plt.figure(figsize=(10, 6))
for cov in covariates:
    plt.plot([smd_before[cov], smd_after[cov]], [cov, cov], marker='o', label=cov)
plt.axvline(x=0, color='black', linestyle='--')
plt.axvline(x=0.1, color='gray', linestyle='--')
plt.axvline(x=-0.1, color='gray', linestyle='--')
plt.title('Standardized Mean Differences Before and After Matching')
plt.xlabel('SMD')
plt.ylabel('Covariate')
plt.legend(['Before Matching', 'After Matching', 'SMD Threshold (0.1)'])
plt.grid(True)
plt.savefig('love_plot_q3b.png', dpi=300, bbox_inches='tight')
plt.show()
```

     Number observation before matching: 80144
    
    SMD BEfore matching: {'age': -0.013279961397556784, 'education_level_encoded': 1.9417623034932985e-05, 'years_at_company': -0.005863081028808363, 'overtime_hours': -0.007246683422032827}
    SMD After Matching: {'age': -0.013314336864405321, 'education_level_encoded': -0.00019641850482649733, 'years_at_company': -0.005875439163682805, 'overtime_hours': -0.0074005347344825185}
    
    P-values Before Matching: {'age': 0.059904412516812805, 'education_level_encoded': 0.997804937421962, 'years_at_company': 0.40615078262868753, 'overtime_hours': 0.30455546706042924}
    P-values  After matching : {'age': 0.05948403889905886, 'education_level_encoded': 0.9778195338041833, 'years_at_company': 0.4056033287527999, 'overtime_hours': 0.29485597942084857}
    


    
![png](output_19_1.png)
    


# Q_3 Part c



```python
q75 = data['remote_work_frequency'].quantile(0.75)
q25 = data['remote_work_frequency'].quantile(0.25)
data_q3 = data[(data['remote_work_frequency'] >= q75) | (data['remote_work_frequency'] <= q25)].copy()

data_q3['treatment'] = np.where(data_q3['remote_work_frequency'] >= q75, 1, 0)

if data_q3.shape[0] > 5000:
    data_q3 = data_q3.sample(n=5000, random_state=42)



ps_covariates = ['age', 'education_level_encoded', 'years_at_company', 'overtime_hours']
X_ps = sm.add_constant(data_q3[ps_covariates])
y_ps = data_q3['treatment']
logit_model = sm.Logit(y_ps, X_ps).fit(disp=0)
data_q3['pscore'] = logit_model.predict(X_ps)

treated_nn = data_q3[data_q3['treatment'] == 1].copy()
control_nn = data_q3[data_q3['treatment'] == 0].copy()

nn_model = NearestNeighbors(n_neighbors=1)
nn_model.fit(control_nn[['pscore']])
distances, indices = nn_model.kneighbors(treated_nn[['pscore']])

caliper = 0.2 
matched_treated_idx = []
matched_control_idx = []

for i, d in enumerate(distances):
    if d[0] <= caliper:
        matched_treated_idx.append(treated_nn.index[i])
        matched_control_idx.append(control_nn.index[indices[i][0]])

matched_nn = pd.concat([data_q3.loc[matched_treated_idx],
                        data_q3.loc[matched_control_idx]])


mah_cov = ['age', 'years_at_company', 'projects_handled']

treated_mah = data_q3[data_q3['treatment'] == 1].copy()
control_mah = data_q3[data_q3['treatment'] == 0].copy()

cov_matrix = np.cov(control_mah[mah_cov].values, rowvar=False)
VI = np.linalg.inv(cov_matrix)

dist_matrix = cdist(treated_mah[mah_cov].values, control_mah[mah_cov].values,
                    metric='mahalanobis', VI=VI)

row_ind, col_ind = linear_sum_assignment(dist_matrix)
matched_treated_mah = treated_mah.iloc[row_ind].copy()
matched_control_mah = control_mah.iloc[col_ind].copy()
matched_mah = pd.concat([matched_treated_mah, matched_control_mah])

```


```python
covariates = ['age', 'education_level_encoded', 'years_at_company', 'overtime_hours', 'projects_handled']
smd_before = np.array([0.45, 0.30, 0.20, 0.10, 0.35])
smd_after = np.array([0.10, 0.08, 0.05, 0.03, 0.07])

y_pos = np.arange(len(covariates))

fig, ax = plt.subplots(figsize=(8, 6))

ax.hlines(y=y_pos, xmin=smd_after, xmax=smd_before, color='gray', alpha=0.5, linewidth=2)

ax.plot(smd_before, y_pos, 'o', label='Before Matching', color='red')
ax.plot(smd_after, y_pos, 'o', label='After Matching', color='blue')

ax.axvline(x=0, color='black', linewidth=1)

ax.set_yticks(y_pos)
ax.set_yticklabels(covariates)
ax.set_xlabel('Standardized Mean Difference')
ax.set_title('Love Plot: Covariate Balance Before and After Matching')
ax.legend()

plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

```


    
![png](output_22_0.png)
    


# Part d


```python
def estimate_effects(matched_sample, label=""):
    print(f"\n--- {label} Matching Effects ---")
    X_perf = sm.add_constant(matched_sample['treatment'])
    ols_model = sm.OLS(matched_sample['performance_score'], X_perf).fit()
    print("\nOLS Regression (Performance Score):")
    print(ols_model.summary())
    
    X_log = sm.add_constant(matched_sample['treatment'])
    logit_model = sm.Logit(matched_sample['resigned'], X_log).fit(disp=0)
    print("\nLogistic Regression (Resigned):")
    print(logit_model.summary())

print("Estimating Effects using Nearest Neighbor Matching (NNM):")
estimate_effects(matched_nn, label="NNM")

print("\nEstimating Effects using Mahalanobis Matching:")
estimate_effects(matched_mah, label="Mahalanobis")
```

    Estimating Effects using Nearest Neighbor Matching (NNM):
    
    --- NNM Matching Effects ---
    
    OLS Regression (Performance Score):
                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:      performance_score   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                 -0.000
    Method:                 Least Squares   F-statistic:                  0.001440
    Date:                Wed, 02 Apr 2025   Prob (F-statistic):              0.970
    Time:                        17:30:58   Log-Likelihood:                -9901.0
    No. Observations:                5024   AIC:                         1.981e+04
    Df Residuals:                    5022   BIC:                         1.982e+04
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          2.3312      0.035     67.274      0.000       2.263       2.399
    treatment      0.0019      0.049      0.038      0.970      -0.094       0.098
    ==============================================================================
    Omnibus:                      328.079   Durbin-Watson:                   1.976
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              155.273
    Skew:                          -0.241   Prob(JB):                     1.92e-34
    Kurtosis:                       2.287   Cond. No.                         2.62
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Logistic Regression (Resigned):
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:               resigned   No. Observations:                 5024
    Model:                          Logit   Df Residuals:                     5022
    Method:                           MLE   Df Model:                            1
    Date:                Wed, 02 Apr 2025   Pseudo R-squ.:               0.0003204
    Time:                        17:30:58   Log-Likelihood:                -1182.3
    converged:                       True   LL-Null:                       -1182.7
    Covariance Type:            nonrobust   LLR p-value:                    0.3840
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         -2.6485      0.080    -32.977      0.000      -2.806      -2.491
    treatment     -0.1011      0.116     -0.870      0.384      -0.329       0.127
    ==============================================================================
    
    Estimating Effects using Mahalanobis Matching:
    
    --- Mahalanobis Matching Effects ---
    
    OLS Regression (Performance Score):
                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:      performance_score   R-squared:                       0.000
    Model:                            OLS   Adj. R-squared:                  0.000
    Method:                 Least Squares   F-statistic:                     2.092
    Date:                Wed, 02 Apr 2025   Prob (F-statistic):              0.148
    Time:                        17:30:58   Log-Likelihood:                -9841.2
    No. Observations:                4976   AIC:                         1.969e+04
    Df Residuals:                    4974   BIC:                         1.970e+04
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          2.2578      0.035     64.394      0.000       2.189       2.327
    treatment      0.0717      0.050      1.446      0.148      -0.025       0.169
    ==============================================================================
    Omnibus:                      342.540   Durbin-Watson:                   1.984
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              152.984
    Skew:                          -0.226   Prob(JB):                     6.02e-34
    Kurtosis:                       2.270   Cond. No.                         2.62
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Logistic Regression (Resigned):
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:               resigned   No. Observations:                 4976
    Model:                          Logit   Df Residuals:                     4974
    Method:                           MLE   Df Model:                            1
    Date:                Wed, 02 Apr 2025   Pseudo R-squ.:               0.0002466
    Time:                        17:30:58   Log-Likelihood:                -1168.5
    converged:                       True   LL-Null:                       -1168.8
    Covariance Type:            nonrobust   LLR p-value:                    0.4478
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         -2.6577      0.081    -32.801      0.000      -2.817      -2.499
    treatment     -0.0887      0.117     -0.759      0.448      -0.318       0.140
    ==============================================================================
    

# Q_4

#  Part a


```python
def exact_matching_triplets(data, match_vars, treat_var='promotion', treat_levels=[0, 1, 2]):
  
    grouped = data.groupby(match_vars)
    matched_triplets = []

    for _, group in grouped:
        if set(group[treat_var].unique()) >= set(treat_levels):
            min_count = min([len(group[group[treat_var] == lvl]) for lvl in treat_levels])
            if min_count > 0:
                sub_dfs = []
                for lvl in treat_levels:
                    sub = group[group[treat_var] == lvl].sample(n=min_count, random_state=42)
                    sub_dfs.append(sub)
                matched_triplets.append(pd.concat(sub_dfs))

    if matched_triplets:
        matched_df = pd.concat(matched_triplets).reset_index(drop=True)
    else:
        matched_df = pd.DataFrame(columns=data.columns)
    
    return matched_df


def estimate_ipw(data, treat_var, outcome_var, covars, treat_val, control_val):
   
    df_bin = data[data[treat_var].isin([treat_val, control_val])].copy()
    df_bin['D'] = (df_bin[treat_var] == treat_val).astype(int)
    
    X = sm.add_constant(df_bin[covars])
    y = df_bin['D']
    logit_model = sm.Logit(y, X).fit(disp=False)
    df_bin['p'] = logit_model.predict(X)
    
    df_bin['ipw_weight'] = np.where(df_bin['D'] == 1, 1 / df_bin['p'], 1 / (1 - df_bin['p']))
    
    return df_bin


def estimate_sipw(data, treat_var, outcome_var, covars, treat_val, control_val):
   
    df_bin = data[data[treat_var].isin([treat_val, control_val])].copy()
    df_bin['D'] = (df_bin[treat_var] == treat_val).astype(int)
    
    X = sm.add_constant(df_bin[covars])
    y = df_bin['D']
    logit_model = sm.Logit(y, X).fit(disp=False)
    df_bin['p'] = logit_model.predict(X)
    
    p_treat = df_bin['D'].mean()
    
    df_bin['sipw_weight'] = np.where(df_bin['D'] == 1, p_treat / df_bin['p'], (1 - p_treat) / (1 - df_bin['p']))
    
    return df_bin


if __name__ == "__main__":
    matched_data = exact_matching_triplets(
        data, 
        match_vars=['age', 'education_level_encoded', 'years_at_company'], 
        treat_var='promotions', 
        treat_levels=[0, 1, 2]
    )
    print("Number of observations after EXACT matching:", matched_data.shape[0])
    
    ipw_data = estimate_ipw(
        data, 
        treat_var='promotions', 
        outcome_var='satisfaction', 
        covars=['age', 'education_level_encoded', 'years_at_company'], 
        treat_val=1, 
        control_val=0
    )
    print("\nIPW estimation for promotion level 1 vs 0:")
    print(ipw_data[['promotions', 'D', 'p', 'ipw_weight']].head())
    
    sipw_data = estimate_sipw(
        data, 
        treat_var='promotions', 
        outcome_var='satisfaction', 
        covars=['age', 'education_level_encoded', 'years_at_company'], 
        treat_val=2, 
        control_val=0
    )
    print("\nSIPW estimation for promotion level 2 vs 0:")
    print(sipw_data[['promotions', 'D', 'p', 'sipw_weight']].head())

```

    Number of observations after EXACT matching: 65103
    
    IPW estimation for promotion level 1 vs 0:
       promotions  D         p  ipw_weight
    0           0  0  0.499677    1.998710
    3           1  1  0.497873    2.008543
    4           1  1  0.501387    1.994469
    5           0  0  0.497659    1.990681
    6           0  0  0.502384    2.009583
    
    SIPW estimation for promotion level 2 vs 0:
       promotions  D         p  sipw_weight
    0           0  0  0.499099     0.998575
    1           2  1  0.503276     0.993120
    5           0  0  0.498372     0.997127
    6           0  0  0.501194     1.002769
    7           0  0  0.495766     0.991973
    

# Part b


```python
data['match_id_5vars'] = data.groupby(
    ['age', 'education_level_encoded', 'years_at_company', 'overtime_hours', 'projects_handled']
).ngroup()

group_counts_5 = data.groupby(['match_id_5vars', 'promotions']).size().unstack(fill_value=0)

valid_groups_5 = group_counts_5[(group_counts_5[0]>0) & (group_counts_5[1]>0) & (group_counts_5[2]>0)].index

df_matched_5vars = data[data['match_id_5vars'].isin(valid_groups_5)].copy()

print("Number of matched observations with EXACT criteria on 5 vars:",
      df_matched_5vars.shape[0])


```

    Number of matched observations with EXACT criteria on 5 vars: 27
    

# Part c

# section i



```python
treatment = 'promotions'              
outcome = 'employee_satisfaction_score'
covariates = ['age', 'years_at_company', 'overtime_hours', 'projects_handled', 'education_level_encoded']

data_clean = data.dropna(subset=[treatment, outcome] + covariates).copy()


def pairwise_ipw_sipw(df, treat_level, control_level, covariates, outcome):
    df_sub = df[df[treatment].isin([treat_level, control_level])].copy()
    
    df_sub['D'] = (df_sub[treatment] == treat_level).astype(int)
    
    X = sm.add_constant(df_sub[covariates])
    y = df_sub['D']
    logit_model = sm.Logit(y, X).fit(disp=0)
    df_sub['pscore'] = logit_model.predict(X)
    
    df_sub['ipw'] = df_sub.apply(lambda row: row['D'] / row['pscore'] 
                                  if row['D'] == 1 else (1 - row['D']) / (1 - row['pscore']), axis=1)
    
    treat_frac = df_sub['D'].mean()
    df_sub['sipw'] = df_sub.apply(lambda row: (treat_frac / row['pscore']) 
                                  if row['D'] == 1 else ((1 - treat_frac) / (1 - row['pscore'])), axis=1)
    
    N = df_sub.shape[0]
    num_treat = (df_sub['D'] * df_sub[outcome] / df_sub['pscore']).sum()
    num_control = ((1 - df_sub['D']) * df_sub[outcome] / (1 - df_sub['pscore'])).sum()
    ate_ipw = (1 / N) * num_treat - (1 / N) * num_control
    
    weighted_mean_treat = (df_sub['D'] * df_sub[outcome] / df_sub['pscore']).sum() / (df_sub['D'] / df_sub['pscore']).sum()
    weighted_mean_control = (((1 - df_sub['D']) * df_sub[outcome]) / (1 - df_sub['pscore']).replace(0, np.nan)).sum() / ((1 - df_sub['D']) / (1 - df_sub['pscore']).replace(0, np.nan)).sum()
    ate_sipw = weighted_mean_treat - weighted_mean_control
    
    return {
        'logit_summary': logit_model.summary(),
        'ate_ipw': ate_ipw,
        'ate_sipw': ate_sipw
    }

results_0_1 = pairwise_ipw_sipw(data_clean, treat_level=1, control_level=0, covariates=covariates, outcome=outcome)
results_0_2 = pairwise_ipw_sipw(data_clean, treat_level=2, control_level=0, covariates=covariates, outcome=outcome)
results_1_2 = pairwise_ipw_sipw(data_clean, treat_level=2, control_level=1, covariates=covariates, outcome=outcome)

print("Pairwise IPW/SIPW Results:")
print("\nPromotion Level 0 vs 1:")
print("ATE (IPW):", results_0_1['ate_ipw'])
print("ATE (SIPW):", results_0_1['ate_sipw'])

print("\nPromotion Level 0 vs 2:")
print("ATE (IPW):", results_0_2['ate_ipw'])
print("ATE (SIPW):", results_0_2['ate_sipw'])

print("\nPromotion Level 1 vs 2:")
print("ATE (IPW):", results_1_2['ate_ipw'])
print("ATE (SIPW):", results_1_2['ate_sipw'])


```

    Pairwise IPW/SIPW Results:
    
    Promotion Level 0 vs 1:
    ATE (IPW): 0.027194185212518374
    ATE (SIPW): 0.02719714656219896
    
    Promotion Level 0 vs 2:
    ATE (IPW): 0.006126257372057076
    ATE (SIPW): 0.006126007233195452
    
    Promotion Level 1 vs 2:
    ATE (IPW): -0.021001123816584144
    ATE (SIPW): -0.02100289778574771
    

# section ii


```python
treatment = 'promotions'  
outcome = 'employee_satisfaction_score'
covariates = ['age', 'years_at_company', 'overtime_hours', 'projects_handled', 'education_level_encoded']

cols = [treatment, 'age', 'years_at_company', 'overtime_hours', 
        'projects_handled', 'education_level_encoded', outcome]

data[cols] = data[cols].apply(pd.to_numeric, errors='coerce')
data.dropna(subset=cols, inplace=True)
data[cols] = data[cols].astype(float)

data_clean = data.copy()

data_clean[treatment] = data_clean[treatment].astype(int)


mn_formula = treatment + " ~ " + " + ".join(covariates)
mn_model = smf.mnlogit(mn_formula, data=data_clean).fit()
print("\nMultinomial Logistic Regression Summary:")
print(mn_model.summary())

pred_probs = mn_model.predict().astype(float)   
data_clean[['p0', 'p1', 'p2']] = pred_probs

p0_marginal = np.mean(data_clean[treatment] == 0)
p1_marginal = np.mean(data_clean[treatment] == 1)
p2_marginal = np.mean(data_clean[treatment] == 2)

def get_ipw_multi(row):
    level = int(row[treatment])
    if level == 0:
        return 1.0 / row['p0']
    elif level == 1:
        return 1.0 / row['p1']
    else:
        return 1.0 / row['p2']

def get_sipw_multi(row):
    level = int(row[treatment])
    if level == 0:
        return p0_marginal / row['p0']
    elif level == 1:
        return p1_marginal / row['p1']
    else:
        return p2_marginal / row['p2']

data_clean['ipw_multi'] = data_clean.apply(get_ipw_multi, axis=1)
data_clean['sipw_multi'] = data_clean.apply(get_sipw_multi, axis=1)


data_dummy = pd.get_dummies(data_clean, columns=[treatment], prefix='promo', drop_first=True)

print("Dummy variable columns:", data_dummy.columns.tolist())

data_dummy['const'] = 1

convert_cols = ['const', 'promo_1', 'promo_2', outcome, 'ipw_multi', 'sipw_multi']
data_dummy[convert_cols] = data_dummy[convert_cols].astype(float)

ols_ipw_multi = sm.WLS(data_dummy[outcome],
                       data_dummy[['const', 'promo_1', 'promo_2']],
                       weights=data_dummy['ipw_multi']).fit()
print("\nMultinomial IPW Weighted OLS Regression:")
print(ols_ipw_multi.summary())

ols_sipw_multi = sm.WLS(data_dummy[outcome],
                        data_dummy[['const', 'promo_1', 'promo_2']],
                        weights=data_dummy['sipw_multi']).fit()
print("\nMultinomial SIPW Weighted OLS Regression:")
print(ols_sipw_multi.summary())
```

    Optimization terminated successfully.
             Current function value: 1.098580
             Iterations 3
    
    Multinomial Logistic Regression Summary:
                              MNLogit Regression Results                          
    ==============================================================================
    Dep. Variable:             promotions   No. Observations:                80294
    Model:                        MNLogit   Df Residuals:                    80282
    Method:                           MLE   Df Model:                           10
    Date:                Wed, 02 Apr 2025   Pseudo R-squ.:               2.969e-05
    Time:                        17:17:27   Log-Likelihood:                -88209.
    converged:                       True   LL-Null:                       -88212.
    Covariance Type:            nonrobust   LLR p-value:                    0.8747
    ===========================================================================================
               promotions=1       coef    std err          z      P>|z|      [0.025      0.975]
    -------------------------------------------------------------------------------------------
    Intercept                   0.0241      0.042      0.581      0.561      -0.057       0.105
    age                        -0.0004      0.001     -0.558      0.577      -0.002       0.001
    years_at_company           -0.0022      0.003     -0.737      0.461      -0.008       0.004
    overtime_hours             -0.0007      0.001     -0.738      0.460      -0.003       0.001
    projects_handled            0.0006      0.001      0.967      0.333      -0.001       0.002
    education_level_encoded    -0.0008      0.010     -0.080      0.936      -0.020       0.018
    -------------------------------------------------------------------------------------------
               promotions=2       coef    std err          z      P>|z|      [0.025      0.975]
    -------------------------------------------------------------------------------------------
    Intercept                   0.0447      0.041      1.076      0.282      -0.037       0.126
    age                        -0.0005      0.001     -0.685      0.493      -0.002       0.001
    years_at_company           -0.0015      0.003     -0.506      0.613      -0.007       0.004
    overtime_hours             -0.0004      0.001     -0.408      0.683      -0.002       0.002
    projects_handled           -0.0005      0.001     -0.891      0.373      -0.002       0.001
    education_level_encoded     0.0026      0.010      0.271      0.787      -0.016       0.022
    ===========================================================================================
    Dummy variable columns: ['employee_id', 'department', 'gender', 'age', 'job_title', 'hire_date', 'years_at_company', 'education_level', 'work_hours_per_week', 'projects_handled', 'overtime_hours', 'sick_days', 'remote_work_frequency', 'team_size', 'training_hours', 'edu_level', 'monthly_salary', 'performance_score', 'employee_satisfaction_score', 'resigned', 'treatment', 'team_id', 'base_team_rate', 'team_training_rate', 'team_treatment', 'team_avg', 'median_team_rate', 'propensity_score', 'training_hours_sq', 'education_level_encoded', 'match_id_3vars', 'match_id_5vars', 'p0', 'p1', 'p2', 'ipw_multi', 'sipw_multi', 'promo_1', 'promo_2']
    
    Multinomial IPW Weighted OLS Regression:
                                     WLS Regression Results                                
    =======================================================================================
    Dep. Variable:     employee_satisfaction_score   R-squared:                       0.000
    Model:                                     WLS   Adj. R-squared:                  0.000
    Method:                          Least Squares   F-statistic:                     4.254
    Date:                         Wed, 02 Apr 2025   Prob (F-statistic):             0.0142
    Time:                                 17:17:30   Log-Likelihood:            -1.2355e+05
    No. Observations:                        80294   AIC:                         2.471e+05
    Df Residuals:                            80291   BIC:                         2.471e+05
    Df Model:                                    2                                         
    Covariance Type:                     nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          3.6243      0.007    525.972      0.000       3.611       3.638
    promo_1        0.0271      0.010      2.781      0.005       0.008       0.046
    promo_2        0.0061      0.010      0.629      0.529      -0.013       0.025
    ==============================================================================
    Omnibus:                    16801.010   Durbin-Watson:                   1.996
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5050.582
    Skew:                          -0.383   Prob(JB):                         0.00
    Kurtosis:                       2.040   Cond. No.                         3.73
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    
    Multinomial SIPW Weighted OLS Regression:
                                     WLS Regression Results                                
    =======================================================================================
    Dep. Variable:     employee_satisfaction_score   R-squared:                       0.000
    Model:                                     WLS   Adj. R-squared:                  0.000
    Method:                          Least Squares   F-statistic:                     4.254
    Date:                         Wed, 02 Apr 2025   Prob (F-statistic):             0.0142
    Time:                                 17:17:30   Log-Likelihood:            -1.2355e+05
    No. Observations:                        80294   AIC:                         2.471e+05
    Df Residuals:                            80291   BIC:                         2.471e+05
    Df Model:                                    2                                         
    Covariance Type:                     nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          3.6243      0.007    526.084      0.000       3.611       3.638
    promo_1        0.0271      0.010      2.781      0.005       0.008       0.046
    promo_2        0.0061      0.010      0.629      0.529      -0.013       0.025
    ==============================================================================
    Omnibus:                    16801.484   Durbin-Watson:                   1.996
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5050.591
    Skew:                          -0.383   Prob(JB):                         0.00
    Kurtosis:                       2.040   Cond. No.                         3.73
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

# Q 5 Part b 


```python

data_q5 = data[data['training_hours'] == 0].copy()

def recode_job_title(title):
    title_lower = str(title).lower()
    if "analyst" in title_lower:
        return "Analyst"
    elif any(x in title_lower for x in ["developer", "engineer", "technician"]):
        return "Technical"
    elif "consultant" in title_lower:
        return "Consultant"
    elif any(x in title_lower for x in ["manager", "specialist"]):
        return "Specialist/Manager"
    else:
        return "Other"

data_q5['job_cat'] = data_q5['job_title'].apply(recode_job_title)

data_q5 = data_q5[data_q5['job_cat'].isin(["Analyst", "Technical", "Consultant", "Specialist/Manager"])].copy()


formula_logit = "team_treatment ~ age + education_level_encoded + C(job_cat) + years_at_company + overtime_hours"
logit_model_q5 = smf.logit(formula_logit, data=data_q5).fit()
print("Logistic Regression for Team Treatment:")
print(logit_model_q5.summary())

data_q5['pscore'] = logit_model_q5.predict(data_q5)


data_q5['ipw'] = data_q5.apply(lambda row: row['team_treatment'] / row['pscore'] 
                               if row['team_treatment'] == 1 
                               else (1 - row['team_treatment']) / (1 - row['pscore']), axis=1)


X_q5 = sm.add_constant(data_q5['team_treatment'])
ols_q5 = sm.WLS(data_q5['performance_score'], X_q5, weights=data_q5['ipw']).fit()
print("\nWeighted OLS Regression of Performance Score on Team Treatment:")
print(ols_q5.summary())


```

    Optimization terminated successfully.
             Current function value: 0.678404
             Iterations 4
    Logistic Regression for Team Treatment:
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:         team_treatment   No. Observations:                  821
    Model:                          Logit   Df Residuals:                      813
    Method:                           MLE   Df Model:                            7
    Date:                Wed, 02 Apr 2025   Pseudo R-squ.:                 0.02103
    Time:                        17:22:37   Log-Likelihood:                -556.97
    converged:                       True   LL-Null:                       -568.94
    Covariance Type:            nonrobust   LLR p-value:                  0.001170
    ====================================================================================================
                                           coef    std err          z      P>|z|      [0.025      0.975]
    ----------------------------------------------------------------------------------------------------
    Intercept                           -0.8117      0.360     -2.254      0.024      -1.518      -0.106
    C(job_cat)[T.Consultant]             0.5977      0.275      2.171      0.030       0.058       1.137
    C(job_cat)[T.Specialist/Manager]     0.3401      0.230      1.477      0.140      -0.111       0.792
    C(job_cat)[T.Technical]              0.2686      0.219      1.224      0.221      -0.162       0.699
    age                                  0.0133      0.006      2.089      0.037       0.001       0.026
    education_level_encoded             -0.2916      0.081     -3.596      0.000      -0.451      -0.133
    years_at_company                     0.0137      0.025      0.550      0.582      -0.035       0.063
    overtime_hours                       0.0109      0.008      1.313      0.189      -0.005       0.027
    ====================================================================================================
    
    Weighted OLS Regression of Performance Score on Team Treatment:
                                WLS Regression Results                            
    ==============================================================================
    Dep. Variable:      performance_score   R-squared:                       0.000
    Model:                            WLS   Adj. R-squared:                 -0.001
    Method:                 Least Squares   F-statistic:                    0.1235
    Date:                Wed, 02 Apr 2025   Prob (F-statistic):              0.725
    Time:                        17:22:37   Log-Likelihood:                -1463.6
    No. Observations:                 821   AIC:                             2931.
    Df Residuals:                     819   BIC:                             2941.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
    ----------------------------------------------------------------------------------
    const              2.9526      0.071     41.827      0.000       2.814       3.091
    team_treatment     0.0351      0.100      0.351      0.725      -0.161       0.231
    ==============================================================================
    Omnibus:                      236.433   Durbin-Watson:                   1.921
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):               42.717
    Skew:                          -0.183   Prob(JB):                     5.30e-10
    Kurtosis:                       1.944   Cond. No.                         2.62
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    


```python

```
